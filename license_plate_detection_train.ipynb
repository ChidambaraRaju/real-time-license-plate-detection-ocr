{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rUubz-LXD6j"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Pipeline\n",
        "dataset = load_dataset(\"justjuu/license-plate-detection\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "M0JCZ6NjXKE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = dataset[\"train\"].features[\"objects\"][\"category\"]\n",
        "id2label = {i: cat for i, cat in enumerate(categories.feature.names)}\n",
        "label2id = {cat: i for i, cat in id2label.items()}\n",
        "COLOR_MAP = {\n",
        "    0: \"GREEN\"\n",
        "}"
      ],
      "metadata": {
        "id": "PfSH5NVMXRfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_random_samples(dataset, split = \"train\", num_sample = 3):\n",
        "  sample_idx = random.sample(range(len(dataset[split])), num_sample)\n",
        "\n",
        "  for idx in sample_idx:\n",
        "    sample = dataset[split][idx]\n",
        "    image = sample[\"image\"]\n",
        "    objects = sample[\"objects\"]\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize = (8, 8))\n",
        "    ax.imshow(image)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    for bbox, label in zip(objects[\"bbox\"], objects[\"category\"]):\n",
        "      x, y, w, h = bbox\n",
        "      rect = patches.Rectangle(\n",
        "          (x, y), w, h,\n",
        "          linewidth = 2,\n",
        "          edgecolor = COLOR_MAP[label],\n",
        "          facecolor = \"none\"\n",
        "      )\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "      ax.text(x, y-5, id2label[label], color = COLOR_MAP[label], fontsize = 10,bbox=dict(facecolor=\"black\", alpha=0.5, pad=1))"
      ],
      "metadata": {
        "id": "RIMnoEAnXbTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_random_samples(dataset)"
      ],
      "metadata": {
        "id": "V7dcTFa_Xcn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
        "\n",
        "MODEL_ID = \"PekingU/rtdetr_v2_r50vd\"\n",
        "model = AutoModelForObjectDetection.from_pretrained(MODEL_ID, id2label = id2label, label2id = label2id, ignore_mismatched_sizes = True)\n",
        "image_processor = AutoImageProcessor.from_pretrained(MODEL_ID, use_fast = True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oSVCjspUXslg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class SingleCOCOAnnotation:\n",
        "  image_id: int\n",
        "  category_id: int\n",
        "  bbox: List[int]\n",
        "\n",
        "@dataclass\n",
        "class ImageAnnotation:\n",
        "  image_id: int\n",
        "  annotations: List[SingleCOCOAnnotation]"
      ],
      "metadata": {
        "id": "VR-16IBzX2bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hf_to_coco_targets(images, objects, image_id):\n",
        "    \"\"\"\n",
        "    Convert Hugging Face object-detection samples to COCO-style\n",
        "    targets required by RT-DETR v2 image processor.\n",
        "\n",
        "    Args:\n",
        "        images (List[PIL.Image]): list of images\n",
        "        objects (List[dict]): list of objects dicts from dataset\n",
        "            each dict contains:\n",
        "                - \"bbox\": List[List[x, y, w, h]]\n",
        "                - \"category\": List[int]\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: COCO-style targets (one per image)\n",
        "    \"\"\"\n",
        "\n",
        "    targets = []\n",
        "\n",
        "    for  img, obj in zip(images, objects):\n",
        "        annotations = []\n",
        "\n",
        "        for bbox, category_id in zip(obj[\"bbox\"], obj[\"category\"]):\n",
        "            x, y, w, h = bbox\n",
        "            area = w * h\n",
        "\n",
        "            annotations.append({\n",
        "                \"image_id\": image_id,\n",
        "                \"bbox\": [x, y, w, h],\n",
        "                \"category_id\": int(category_id),\n",
        "                \"area\": float(area),\n",
        "                \"iscrowd\": 0\n",
        "            })\n",
        "\n",
        "        targets.append({\n",
        "            \"image_id\": image_id,\n",
        "            \"annotations\": annotations\n",
        "        })\n",
        "\n",
        "    return targets"
      ],
      "metadata": {
        "id": "CImcomk8YKw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx = random.randint(0, len(dataset[\"train\"]))\n",
        "random_image = dataset[\"train\"][random_idx][\"image\"]\n",
        "random_objects = dataset[\"train\"][random_idx][\"objects\"]"
      ],
      "metadata": {
        "id": "x02Kf7NcYRxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample_coco_annotation = hf_to_coco_targets([random_image], [random_objects], 1)\n",
        "random_sample_coco_annotation"
      ],
      "metadata": {
        "id": "xPIgUIaMYU6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample_preprocessed = image_processor.preprocess(images=random_image, annotations=random_sample_coco_annotation, return_tensors = \"pt\")\n",
        "random_sample_preprocessed"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ecyBxcsDYXWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(batch, image_processor=image_processor):\n",
        "  images = []\n",
        "  coco_annotations = []\n",
        "  for image_id, (image, objects) in enumerate(zip(batch[\"image\"], batch[\"objects\"])):\n",
        "    images.append(image)\n",
        "    coco_annotations.extend(hf_to_coco_targets([image], [objects], image_id))\n",
        "\n",
        "  processed_batch = image_processor.preprocess(images = images, annotations = coco_annotations, return_tensors = \"pt\")\n",
        "  return processed_batch"
      ],
      "metadata": {
        "id": "8fzo92d3YbKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"] = dataset[\"train\"].with_transform(preprocess_batch)\n",
        "dataset[\"validation\"] = dataset[\"validation\"].with_transform(preprocess_batch)\n",
        "dataset[\"test\"] = dataset[\"test\"].with_transform(preprocess_batch)"
      ],
      "metadata": {
        "id": "nBA6n7dtZgQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "def data_collate_function(preprocessed_batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"Stacks together groups of preprocessed samples into batches for our model.\n",
        "\n",
        "    Args:\n",
        "        preprocessed_batch: A list of dictionaries where each dictionary represnets a preprocessed sample.\n",
        "\n",
        "    Returns:\n",
        "        collated_data: A dictionary containing the batched data ready in the format our model\n",
        "            is expecting. The dictionary has the following keys:\n",
        "                - \"pixel_values\": A stacked tensor of preprocessed pixel values.\n",
        "                - \"labels\": A list of label dictionaries.\n",
        "                - \"pixel_mask\": (Optional) A stacked tensor of pixel masks (this will be present\n",
        "                    only if the input contains a \"pixel_mask\" key.\n",
        "    \"\"\"\n",
        "    # Create an empty dictionary (our model wants a dictionary input)\n",
        "    collated_data = {}\n",
        "\n",
        "    # Stack together a collection of pixel_values tensors\n",
        "    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in preprocessed_batch])\n",
        "\n",
        "    # Get the labels (these are dictionaries so no need to use torch.stack)\n",
        "    collated_data[\"labels\"] = [sample[\"labels\"] for sample in preprocessed_batch]\n",
        "\n",
        "    # If there is a pixel_mask key, return the pixel_mask's as well\n",
        "    if \"pixel_mask\" in preprocessed_batch[0]:\n",
        "        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in preprocessed_batch])\n",
        "\n",
        "    return collated_data"
      ],
      "metadata": {
        "id": "1LyQb7D7Ygmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "BATCH_SIZE = 12\n",
        "DATALOADER_NUM_WORKERS = 0 # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0\n",
        "\n",
        "# Set number of epochs to how many laps you'd like to do over the data\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "# Setup hyperameters for training from the DETR paper(s)\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "MAX_GRAD_NORM = 0.1\n",
        "WARMUP_RATIO = 0.05 # learning rate warmup from 0 to learning_rate as a ratio of total steps (e.g. 0.05 = 5% of total steps)\n",
        "\n",
        "# Create directory to save models to\n",
        "OUTPUT_DIR = \"rtdetr_v2_r50vd-v1\"\n",
        "print(f\"[INFO] Saving model to: {OUTPUT_DIR}\")\n",
        "\n",
        "# Create TrainingArguments to pass to Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    # warmup_steps=2000, # number of warmup steps from 0 to learning_rate (overrides warmup_ratio, found this to be too long for our dataset)\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True, # use mixed precision training\n",
        "    dataloader_num_workers=DATALOADER_NUM_WORKERS, # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False, # want to minimize eval_loss (e.g. lower is better)\n",
        "    report_to=\"none\", # don't save experiments to a third party service\n",
        "    push_to_hub=False,\n",
        "    eval_do_concat_batches=False, # this defaults to True but we'll set it to False for our evaluation function\n",
        "    # save_safetensors=False # turn this off to prevent potential checkpoint issues\n",
        ")"
      ],
      "metadata": {
        "id": "JOXqqwv5YsTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collate_function,\n",
        "    train_dataset = dataset['train'],\n",
        "    eval_dataset = dataset['validation'],\n",
        "    compute_metrics = None\n",
        ")\n",
        "training_results = trainer.train()"
      ],
      "metadata": {
        "id": "SzQ3_Q2vYxjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pushing the Model to Huggingface\n",
        "HF_REPO_ID = \"justjuu/rtdetr-v2-license-plate-detection\"\n",
        "model.config.id2label = {0: \"license_plate\"}\n",
        "model.config.label2id = {\"license_plate\": 0}\n",
        "model.push_to_hub(HF_REPO_ID)\n",
        "image_processor.push_to_hub(HF_REPO_ID)"
      ],
      "metadata": {
        "id": "Ere9zBTjgzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
        "\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    HF_REPO_ID\n",
        ")\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    HF_REPO_ID\n",
        ")"
      ],
      "metadata": {
        "id": "R-9BcXXW5q5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "image = Image.open(\"/content/500_4148.jpg\").convert(\"RGB\")\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\n",
        "    \"justjuu/rtdetr-v2-license-plate-detection\"\n",
        ")\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    \"justjuu/rtdetr-v2-license-plate-detection\"\n",
        ")\n",
        "\n",
        "inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "results = processor.post_process_object_detection(\n",
        "    outputs,\n",
        "    target_sizes=torch.tensor([(image.height, image.width)]),\n",
        "    threshold=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "FEVZLlRl6j4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "dsKtxMl-7Drx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQue7F6R7JIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}